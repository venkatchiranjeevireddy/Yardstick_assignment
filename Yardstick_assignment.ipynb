{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtF4g2rVKg/ebZuzbHgTLr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatchiranjeevireddy/Yardstick_assignment/blob/main/Yardstick_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 1: Conversation Management with Summarization (Groq API)\n",
        "\n",
        "This notebook is my solution for **Task 1** of the assignment.  \n",
        "The goal here is to show how we can:\n",
        "\n",
        "Keep track of a conversation history (user ↔ assistant)  \n",
        "Summarize the history every few turns to keep it short and clean  \n",
        "Provide options to truncate history by number of turns or word count  \n",
        "Use **Groq API** with **OpenAI SDK compatibility**  \n",
        "\n",
        "I wrote this notebook step by step, so you can follow along like a story.  \n",
        "Each code cell has an explanation written in plain English so anyone can understand what's happening.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GTb90ZsfTfvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 1: Install the required library\n",
        "\n",
        "We only need the `openai` Python library to talk to Groq’s API.  \n",
        "This step makes sure the library is available inside Google Colab.  \n"
      ],
      "metadata": {
        "id": "IaEn-FaTTuew"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "OVZJtYasPdi8"
      },
      "outputs": [],
      "source": [
        "!pip install openai --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Connect to Groq API\n",
        "\n",
        "Here we connect to the Groq API using the OpenAI-compatible client.  \n",
        "This way, we can use Groq just like we’d use OpenAI.\n"
      ],
      "metadata": {
        "id": "6WouJt7lT1m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "# Configure OpenAI client for Groq API\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=api_key\n",
        ")"
      ],
      "metadata": {
        "id": "xej7zcjtQlET"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Conversation History Setup\n",
        "Here I create a list to store the whole conversation.  \n",
        "Every message has two parts:\n",
        "- **role** (user, assistant, or system)\n",
        "- **content** (the actual text)\n",
        "This is just like a chat app storing who said what.\n"
      ],
      "metadata": {
        "id": "w73yb0X4T-o4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the conversation history as an empty list\n",
        "conversation_history = []\n",
        "def add_message(role, content):\n",
        "    \"\"\"Adds a new message to the conversation history.\"\"\"\n",
        "    conversation_history.append({\"role\": role, \"content\": content})\n",
        "def get_conversation():\n",
        "    \"\"\"Return the current conversation history.\"\"\"\n",
        "    return conversation_history\n"
      ],
      "metadata": {
        "id": "RTIItr2MRRDl"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4: Summarization Function\n",
        "\n",
        "This function takes the entire conversation and asks the Groq model to **summarize** it.  \n",
        "Why? Because if the conversation becomes too long, we don’t want to keep sending the entire text.  \n",
        "Instead, we can keep a short version (summary) that still captures the meaning.  \n"
      ],
      "metadata": {
        "id": "SrsyiQ6GUCi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_conversation(conversation):\n",
        "    \"\"\"Summarize the conversation using Groq API.\"\"\"\n",
        "    prompt = \"Summarize the following conversation:\\n\\n\"\n",
        "    for msg in conversation:\n",
        "        prompt += f\"{msg['role'].capitalize()}: {msg['content']}\\n\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"openai/gpt-oss-120b\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes conversations.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=150\n",
        "    )\n",
        "    summary = response.choices[0].message.content\n",
        "    return summary\n"
      ],
      "metadata": {
        "id": "BD3ub2FDRW9_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Truncation Options\n",
        "\n",
        "Sometimes we don’t want the full conversation.  \n",
        "Here I show two ways to cut it down:\n",
        "1. **By turns** → Keep only the last N messages.  \n",
        "2. **By word count** → Keep only up to N words (most recent words).  \n",
        "\n",
        "This is super useful for keeping the chat light and efficient.\n"
      ],
      "metadata": {
        "id": "yyWzlaM6UNOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_by_turns(conversation, n):\n",
        "    \"\"\"Keep only the last n messages.\"\"\"\n",
        "    return conversation[-n:] if len(conversation) > n else conversation\n"
      ],
      "metadata": {
        "id": "1vy2ZMNgUVDM"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_by_word_count(conversation, max_words):\n",
        "    \"\"\"Truncate the conversation to a maximum number of words.\"\"\"\n",
        "    total_words = 0\n",
        "    truncated = []\n",
        "    # Go through messages from last to first\n",
        "    for msg in reversed(conversation):\n",
        "        word_count = len(msg['content'].split())\n",
        "        if total_words + word_count <= max_words:\n",
        "            truncated.insert(0, msg)\n",
        "            total_words += word_count\n",
        "        else:\n",
        "            break\n",
        "    return truncated\n"
      ],
      "metadata": {
        "id": "nWCMjVkuRepK"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 6: Periodic Summarization\n",
        "\n",
        "Instead of summarizing every single turn, we can do it **every k turns**.  \n",
        "For example: summarize after every 3 turns.  \n",
        "This way, the conversation stays fresh without losing too much detail.\n"
      ],
      "metadata": {
        "id": "wxBARBxgUfdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maybe_summarize(turn_count, k):\n",
        "    \"\"\"Check if summarization should happen after every k turns.\"\"\"\n",
        "    return turn_count % k == 0\n",
        "def apply_periodic_summarization(conversation, turn_count, k=3):\n",
        "    \"\"\"Perform summarization every k turns and replace history with summary.\"\"\"\n",
        "    if maybe_summarize(turn_count, k):\n",
        "        summary = summarize_conversation(conversation)\n",
        "        return [{\"role\": \"system\", \"content\": f\"Summary of conversation:\\n{summary}\"}]\n",
        "    return conversation\n"
      ],
      "metadata": {
        "id": "It3COKZwRhR3"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "rjOAwv2NRnTl"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 7: Example Conversation Demo\n",
        "\n",
        "Now let’s test everything with a fake conversation.  \n",
        "I made up some turns between a user and the assistant.  \n",
        "Watch how the conversation history changes:  \n",
        "- At normal turns, it just adds the message.  \n",
        "- After every 3 turns, it replaces the history with a neat summary.  \n"
      ],
      "metadata": {
        "id": "91KVCV6yUlhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset conversation history\n",
        "conversation_history = []\n",
        "k = 3  # Summarize after every 3 turns\n",
        "# Example conversation turns\n",
        "messages = [\n",
        "    (\"user\", \"Hello!\"),\n",
        "    (\"assistant\", \"Hi there! How can I help you?\"),\n",
        "    (\"user\", \"I want to learn about AI.\"),\n",
        "    (\"assistant\", \"AI is the simulation of human intelligence in machines.\"),\n",
        "    (\"user\", \"Can you explain machine learning?\"),\n",
        "    (\"assistant\", \"Machine learning allows systems to learn from data.\"),\n",
        "]\n",
        "# Simulate conversation turns\n",
        "for i, (role, content) in enumerate(messages, start=1):\n",
        "    add_message(role, content)\n",
        "    print(f\"\\nTurn {i}: {role} says: {content}\")\n",
        "    conversation_history = apply_periodic_summarization(conversation_history, i, k)\n",
        "    print(\"Current Conversation:\")\n",
        "    print(json.dumps(conversation_history, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0XpJsipRk5Z",
        "outputId": "1a27179b-d27e-4001-e720-1a20976d80f7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Turn 1: user says: Hello!\n",
            "Current Conversation:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"Hello!\"\n",
            "  }\n",
            "]\n",
            "\n",
            "Turn 2: assistant says: Hi there! How can I help you?\n",
            "Current Conversation:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"Hello!\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"assistant\",\n",
            "    \"content\": \"Hi there! How can I help you?\"\n",
            "  }\n",
            "]\n",
            "\n",
            "Turn 3: user says: I want to learn about AI.\n",
            "Current Conversation:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"Summary of conversation:\\nThe user greeted the assistant, the assistant responded with a friendly greeting and offered help, and the user expressed interest in learning about AI.\"\n",
            "  }\n",
            "]\n",
            "\n",
            "Turn 4: assistant says: AI is the simulation of human intelligence in machines.\n",
            "Current Conversation:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"Summary of conversation:\\nThe user greeted the assistant, the assistant responded with a friendly greeting and offered help, and the user expressed interest in learning about AI.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"assistant\",\n",
            "    \"content\": \"AI is the simulation of human intelligence in machines.\"\n",
            "  }\n",
            "]\n",
            "\n",
            "Turn 5: user says: Can you explain machine learning?\n",
            "Current Conversation:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"Summary of conversation:\\nThe user greeted the assistant, the assistant responded with a friendly greeting and offered help, and the user expressed interest in learning about AI.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"assistant\",\n",
            "    \"content\": \"AI is the simulation of human intelligence in machines.\"\n",
            "  },\n",
            "  {\n",
            "    \"role\": \"user\",\n",
            "    \"content\": \"Can you explain machine learning?\"\n",
            "  }\n",
            "]\n",
            "\n",
            "Turn 6: assistant says: Machine learning allows systems to learn from data.\n",
            "Current Conversation:\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"Summary of conversation:\\nThe user greeted the assistant, who replied warmly and offered help. The user expressed interest in learning about AI. The assistant explained that AI is the simulation of human intelligence in machines. When the user asked about machine learning, the assistant clarified that machine learning enables systems to learn from data.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 8: Truncation in Action\n",
        "\n",
        "Finally, I show how we can cut the conversation history:  \n",
        "- Only keep the last 3 turns  \n",
        "- Or keep only up to 20 words  \n",
        "\n",
        "This is proof that both truncation methods work.  \n"
      ],
      "metadata": {
        "id": "nEMi3pCsUtVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Truncate by Last 3 Turns\")\n",
        "truncated_turns = truncate_by_turns(conversation_history, 3)\n",
        "print(json.dumps(truncated_turns, indent=2))\n",
        "print(\"\\nTruncate by Word Count (max 20 words)\")\n",
        "truncated_words = truncate_by_word_count(conversation_history, 20)\n",
        "print(json.dumps(truncated_words, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L8yOyYVSp0Y",
        "outputId": "8ea64ee3-8f99-41b9-c134-3b0b430f72d3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Truncate by Last 3 Turns\n",
            "[\n",
            "  {\n",
            "    \"role\": \"system\",\n",
            "    \"content\": \"Summary of conversation:\\nThe user greeted the assistant, who replied warmly and offered help. The user expressed interest in learning about AI. The assistant explained that AI is the simulation of human intelligence in machines. When the user asked about machine learning, the assistant clarified that machine learning enables systems to learn from data.\"\n",
            "  }\n",
            "]\n",
            "\n",
            "Truncate by Word Count (max 20 words)\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, I successfully built a **conversation manager** using Groq API.  \n",
        "Here’s what I achieved:\n",
        "\n",
        "- Tracked the entire chat history  \n",
        "- Summarized the chat periodically (every k turns)  \n",
        "- Provided truncation options (by turns and by word count)  \n",
        "- Kept the implementation clean and easy to follow  \n",
        "\n",
        "This solution is production-friendly because:\n",
        "- It prevents conversations from getting too long  \n",
        "- It saves tokens and cost by keeping only summaries  \n",
        "- It makes the system efficient and smart  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZL5XzzKOU1OS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 – JSON Schema Classification & Information Extraction\n",
        "\n",
        "\n",
        "In this task, we are going to extract important details from user chats using structured data. The idea is to take real conversations and automatically pull out specific information like:\n",
        "\n",
        "- Name  \n",
        "- Email  \n",
        "- Phone number  \n",
        "- Location  \n",
        "- Age\n",
        "\n",
        "We will use a JSON schema to define how this information should be extracted and validated.\n",
        "\n",
        "We’ll also use OpenAI’s function calling feature with Groq API, allowing us to process these chats in a structured way. Don't worry if this sounds a bit technical – we’ll go through everything step-by-step with lots of explanations!\n",
        "\n"
      ],
      "metadata": {
        "id": "IAMHX_eYWAya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Connect to Groq API\n",
        "\n",
        "Here we connect to the Groq API using the OpenAI-compatible client.  \n",
        "This way, we can use Groq just like we’d use OpenAI.\n"
      ],
      "metadata": {
        "id": "pDYY--BPXF7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "from openai import OpenAI\n",
        "import json\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_c1QeENbS9p",
        "outputId": "51dd322e-d4e3-426e-d94e-4c7eea4e50e8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.106.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "# Use Colab's user_data to securely store the API key\n",
        "from google.colab import userdata\n",
        "# Securely input your Groq API key\n",
        "api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "# Configure OpenAI client for Groq API\n",
        "client = openai.OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=api_key\n",
        ")"
      ],
      "metadata": {
        "id": "no1k8a6VU8JO"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining the JSON Schema\n",
        "\n",
        "Now we’ll create a JSON schema that tells the model exactly what information we want to extract from the chat. This schema acts like a template or a guide.\n",
        "\n",
        "We’ll ask for the following details:\n",
        "1. **Name** – The person's name.\n",
        "2. **Email** – Their email address.\n",
        "3. **Phone** – Their phone number.\n",
        "4. **Location** – Where they are from or currently live.\n",
        "5. **Age** – Their age or approximate age.\n",
        "\n",
        "The schema helps the model return consistent and structured data every time!\n"
      ],
      "metadata": {
        "id": "Q5k-KzpdXuim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the JSON schema for extracting required information\n",
        "schema = {\n",
        "    \"name\": \"extract_user_info\",\n",
        "    \"description\": \"Extract user details from chat messages.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"name\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The full name of the user.\"\n",
        "            },\n",
        "            \"email\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The email address of the user.\"\n",
        "            },\n",
        "            \"phone\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The phone number of the user.\"\n",
        "            },\n",
        "            \"location\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The location of the user.\"\n",
        "            },\n",
        "            \"age\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"The age or approximate age of the user.\"\n",
        "            }\n",
        "        },\n",
        "        \"required\": [\"name\", \"email\", \"phone\", \"location\", \"age\"]\n",
        "    }\n",
        "}\n",
        "print(\"JSON schema defined successfully\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--GXGfEeXuVj",
        "outputId": "de8254bc-7b08-44aa-9add-20ba24c01f6d"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON schema defined successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Function to Call the API and Extract Information\n",
        "\n",
        "Let's write a Python function that takes a chat message and uses the model to extract information based on the schema we just defined.\n",
        "\n",
        "The function will:\n",
        "1. Send the user’s message to the API.\n",
        "2. Ask the model to extract details according to our schema.\n",
        "3. Return the structured output.\n",
        "\n",
        "This way, every time we pass a chat, the model will understand it and give us the details we need!\n"
      ],
      "metadata": {
        "id": "Z77vYFkcX5Fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_information(user_message):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"openai/gpt-oss-120b\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an information extractor.\"},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ],\n",
        "            functions=[schema],\n",
        "            function_call={\"name\": \"extract_user_info\"}\n",
        "        )\n",
        "        # Parse function call arguments\n",
        "        function_args = response.choices[0].message.function_call.arguments\n",
        "        return json.loads(function_args)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "58L0Rjd6X3PT"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing the Function with Example Chats\n",
        "\n",
        "Now let's see this in action! We’ll pass a few sample chats where users introduce themselves, share their contact information, and talk about their location or age.\n",
        "\n",
        "We expect the model to parse this information and return it neatly organized according to the schema.\n",
        "\n",
        "Let’s test it with 3 examples.\n"
      ],
      "metadata": {
        "id": "QVoFiXWRZFHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    \"Hi, I’m Venkat, my email is venkat@example.com, phone 9876543210, I live in Hyderabad, age 21.\",\n",
        "    \"Hello, I’m Aarti. You can reach me at aarti123@gmail.com, call me at 9123456789. I stay in Mumbai. I’m 25 years old.\",\n",
        "    \"My name is Rohit Sharma. Email: rohit.s@gmail.com. Phone: 9000011111. Currently in Delhi. I’m 30.\"\n",
        "]\n",
        "for i, chat in enumerate(examples, 1):\n",
        "    print(f\"\\n Example {i} \")\n",
        "    result = extract_information(chat)\n",
        "    print(json.dumps(result, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V46SxSpGY97q",
        "outputId": "3cc550b1-8f27-41d7-80b8-d9067993bded"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Example 1 \n",
            "{\n",
            "  \"age\": \"21\",\n",
            "  \"email\": \"venkat@example.com\",\n",
            "  \"location\": \"Hyderabad\",\n",
            "  \"name\": \"Venkat\",\n",
            "  \"phone\": \"9876543210\"\n",
            "}\n",
            "\n",
            " Example 2 \n",
            "{\n",
            "  \"age\": \"25\",\n",
            "  \"email\": \"aarti123@gmail.com\",\n",
            "  \"location\": \"Mumbai\",\n",
            "  \"name\": \"Aarti\",\n",
            "  \"phone\": \"9123456789\"\n",
            "}\n",
            "\n",
            " Example 3 \n",
            "{\n",
            "  \"age\": \"30\",\n",
            "  \"email\": \"rohit.s@gmail.com\",\n",
            "  \"location\": \"Delhi\",\n",
            "  \"name\": \"Rohit Sharma\",\n",
            "  \"phone\": \"9000011111\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Validating the Extracted Data\n",
        "\n",
        "Let's make sure the model’s output matches the schema. We expect all fields — name, email, phone, location, and age — to be filled.\n",
        "\n",
        "If anything is missing or incorrect, it will be easier to debug and fix it right away.\n",
        "\n",
        "Let's print out a confirmation message for each result.\n"
      ],
      "metadata": {
        "id": "M7sg9FWFZPZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if all required fields are present\n",
        "for i, chat in enumerate(examples, start=1):\n",
        "    print(f\"Validation for Example {i}:\")\n",
        "    extracted_info = extract_information(chat)\n",
        "    if extracted_info:\n",
        "        missing = [key for key in schema['parameters']['required'] if key not in extracted_info]\n",
        "        if missing:\n",
        "            print(f\"Missing fields: {missing}\")\n",
        "        else:\n",
        "            print(\"All required fields are extracted correctly!\")\n",
        "            print(json.dumps(extracted_info, indent=2))\n",
        "    else:\n",
        "        print(\"Extraction failed.\")\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3wRjvVgZMsP",
        "outputId": "51eff91c-90b3-4090-9aa3-2b6d8d558a11"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation for Example 1:\n",
            "All required fields are extracted correctly!\n",
            "{\n",
            "  \"age\": \"21\",\n",
            "  \"email\": \"venkat@example.com\",\n",
            "  \"location\": \"Hyderabad\",\n",
            "  \"name\": \"Venkat\",\n",
            "  \"phone\": \"9876543210\"\n",
            "}\n",
            "\n",
            "==================================================\n",
            "\n",
            "Validation for Example 2:\n",
            "All required fields are extracted correctly!\n",
            "{\n",
            "  \"age\": \"25\",\n",
            "  \"email\": \"aarti123@gmail.com\",\n",
            "  \"location\": \"Mumbai\",\n",
            "  \"name\": \"Aarti\",\n",
            "  \"phone\": \"9123456789\"\n",
            "}\n",
            "\n",
            "==================================================\n",
            "\n",
            "Validation for Example 3:\n",
            "All required fields are extracted correctly!\n",
            "{\n",
            "  \"age\": \"30\",\n",
            "  \"email\": \"rohit.s@gmail.com\",\n",
            "  \"location\": \"Delhi\",\n",
            "  \"name\": \"Rohit Sharma\",\n",
            "  \"phone\": \"9000011111\"\n",
            "}\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Task 2 Complete\n",
        "\n",
        "\n",
        "Defined a JSON schema for extracting user details.  \n",
        "Used Groq's API with OpenAI function calling to parse real chat messages.  \n",
        "Structured and validated the extracted data according to the schema.  \n",
        "\n",
        "This structured approach ensures that even casual conversations can be processed into neat, usable data formats.\n",
        "\n",
        "With this, Task 2 is complete!Next, we can push this notebook to GitHub and prepare the final submission.\n",
        "\n",
        "Let me know when you're ready to proceed with uploading and submitting everything!\n"
      ],
      "metadata": {
        "id": "j2-wTNwTZUXY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-1OJmB7jgAev"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}